name: CI
on:
  push:
  pull_request:
  workflow_dispatch:
    inputs:
      rollback_model:
        description: MLflow model name to rollback
        required: false
        default: bi-encoder
      rollback_version:
        description: Version to set to Production
        required: false
        default: ""

jobs:
  test:
    runs-on: self-hosted
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_URL }}
      MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_USER }}
      MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_PASS }}
      MLFLOW_MODEL_NAME: bi-encoder
      MLFLOW_MODEL_REF: models:/bi-encoder@Staging
      VENV: ${{ github.workspace }}/.venv
    steps:
      - uses: actions/checkout@v4

      - name: Show Python
        run: python3 -V

      - name: Set up venv
        run: |
          python3 -m venv "$VENV"
          "$VENV/bin/python" -m pip install -U pip

      - name: Lint (ruff, soft)
        run: |
          "$VENV/bin/python" -m pip install -U ruff
          # Hard-fail on syntax/undefined names etc.
          "$VENV/bin/python" -m ruff check . --select E9,F63,F7,F82

      - name: Parser tests
        working-directory: AI/services/parser
        run: |
          "$VENV/bin/python" -m pip install -r requirements.txt
          "$VENV/bin/python" -m pip install pytest
          "$VENV/bin/python" -m pytest -q -m "not slow"

      - name: API tests
        working-directory: AI/services/api
        run: |
          "$VENV/bin/python" -m pip install -r requirements.txt
          "$VENV/bin/python" -m pip install pytest
          "$VENV/bin/python" -m pytest -q -m "not slow"

      - name: UI tests (optional)
        working-directory: AI/services/ui
        run: |
          "$VENV/bin/python" -m pip install -r requirements.txt || true
          "$VENV/bin/python" -m pip install pytest || true
          "$VENV/bin/python" -m pytest -q -m "not slow" || true

      - name: Generate parsed.parquet from raw (if present)
        if: ${{ hashFiles('AI/data/raw/**/*.json') != '' && hashFiles('AI/data/interim/parsed.parquet') == '' }}
        working-directory: AI/services/parser
        run: |
          "$VENV/bin/python" -m pip install -q -r requirements.txt
          "$VENV/bin/python" AI/services/parser/parse_guidelines.py --raw AI/data/raw --out AI/data/interim

      - name: Create tiny parsed.parquet fixture (fallback)
        if: ${{ hashFiles('AI/data/interim/parsed.parquet') == '' }}
        run: |
          "$VENV/bin/python" -m pip install -q pandas pyarrow
          "$VENV/bin/python" - <<'PY'
          import pandas as pd, os
          os.makedirs('AI/data/interim', exist_ok=True)
          df = pd.DataFrame([
            {"guideline_id":"g1","group_seq":1,"pos":0,"item_id":"item1","left_neighbor":None,"right_neighbor":"item2"},
            {"guideline_id":"g1","group_seq":1,"pos":1,"item_id":"item2","left_neighbor":"item1","right_neighbor":"item3"},
            {"guideline_id":"g1","group_seq":1,"pos":2,"item_id":"item3","left_neighbor":"item2","right_neighbor":None},
          ])
          df.to_parquet('AI/data/interim/parsed.parquet')
          PY

      - name: Download candidate model from MLflow (optional)
        id: pullmodel
        if: ${{ env.MLFLOW_TRACKING_URI != '' }}
        run: |
          "$VENV/bin/python" -m pip install -q mlflow
          rm -rf AI/models/artifacts || true
          "$VENV/bin/python" - <<'PY'
          import os, mlflow
          dst = 'AI/models/artifacts'
          os.makedirs(dst, exist_ok=True)
          ref = os.environ.get('MLFLOW_MODEL_REF', 'models:/bi-encoder@Staging')
          ok = False
          try:
              mlflow.artifacts.download_artifacts(artifact_uri=ref, dst_path=dst)
              ok = True
              print('Downloaded model artifacts from', ref, 'to', dst)
          except Exception as e:
              print('Model download skipped:', e)
          open('has_model.txt','w').write('true' if ok else 'false')
          PY
          echo "has_model=$(cat has_model.txt)" >> $GITHUB_OUTPUT

      - name: Evaluate bi-encoder (optional)
        id: eval
        if: ${{ steps.pullmodel.outputs.has_model == 'true' && hashFiles('AI/data/interim/parsed.parquet') != '' }}
        run: |
          "$VENV/bin/python" -m pip install -q faiss-cpu pandas numpy pyarrow
          "$VENV/bin/python" AI/services/trainer-bienc/evaluate_bi.py --in AI/data/interim --artifacts AI/models/artifacts --k 1,3,5,10 | tee eval.txt

      - name: Gate metrics (optional)
        id: metrics
        if: ${{ steps.eval.outcome == 'success' }}
        run: |
          "$VENV/bin/python" - <<'PY'
          import re, ast, sys
          txt=open('eval.txt','r',encoding='utf-8').read()
          m=re.search(r'Aggregate: (\{.*\})', txt)
          agg=ast.literal_eval(m.group(1)) if m else {}
          print('Aggregate:', agg)
          ok = (agg.get('recall@1',0)>=0.35 and agg.get('mrr',0)>=0.53)
          print('OK=', ok)
          sys.exit(0 if ok else 1)
          PY

      - name: Set gate output
        id: gate
        run: |
          if [ "${{ steps.metrics.outcome }}" = "success" ]; then echo "ok=true" >> $GITHUB_OUTPUT; else echo "ok=false" >> $GITHUB_OUTPUT; fi
    outputs:
      metrics_ok: ${{ steps.gate.outputs.ok }}

  docker-build:
    runs-on: self-hosted
    needs: test
    steps:
      - uses: actions/checkout@v4
      - name: Configure buildx and cache
        run: |
          docker buildx use ci-builder >/dev/null 2>&1 || docker buildx create --name ci-builder --use
          mkdir -p "$HOME/.cache/buildx"
      - name: Build images (cached)
        run: |
          docker buildx build \
            --cache-from type=local,src=$HOME/.cache/buildx \
            --cache-to   type=local,dest=$HOME/.cache/buildx,mode=max \
            -t ai-parser AI/services/parser --load
          docker buildx build \
            --cache-from type=local,src=$HOME/.cache/buildx \
            --cache-to   type=local,dest=$HOME/.cache/buildx,mode=max \
            -t ai-trainer-bienc AI/services/trainer-bienc --load
          docker buildx build \
            --cache-from type=local,src=$HOME/.cache/buildx \
            --cache-to   type=local,dest=$HOME/.cache/buildx,mode=max \
            -t ai-trainer-gnn AI/services/trainer-gnn --load
          docker buildx build \
            --cache-from type=local,src=$HOME/.cache/buildx \
            --cache-to   type=local,dest=$HOME/.cache/buildx,mode=max \
            -t ai-featurizer AI/services/featurizer --load
          docker buildx build \
            --cache-from type=local,src=$HOME/.cache/buildx \
            --cache-to   type=local,dest=$HOME/.cache/buildx,mode=max \
            -t ai-api AI/services/api --load
          docker buildx build \
            --cache-from type=local,src=$HOME/.cache/buildx \
            --cache-to   type=local,dest=$HOME/.cache/buildx,mode=max \
            -t ai-ui AI/services/ui --load

  promote-model:
    runs-on: self-hosted
    needs: test
    if: ${{ github.ref == 'refs/heads/main' && needs.test.outputs.metrics_ok == 'true' }}
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_URL }}
      MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_USER }}
      MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_PASS }}
      MLFLOW_ALLOW_PROMOTE: ${{ secrets.MLFLOW_ALLOW_PROMOTE }}
      MLFLOW_MODEL_NAME: bi-encoder
      FROM_STAGE: Staging
      TO_STAGE: Production
    steps:
      - uses: actions/checkout@v4
      - name: Show Python
        run: python3 -V
      - name: Set up venv
        run: |
          python3 -m venv .venv
          .venv/bin/python -m pip install -U pip
      - name: Promote latest Staging to Production
        if: ${{ env.MLFLOW_TRACKING_URI != '' && env.MLFLOW_ALLOW_PROMOTE == 'true' }}
        run: |
          .venv/bin/python -m pip install -q mlflow
          .venv/bin/python - <<'PY'
          import os
          from mlflow.tracking import MlflowClient
          client = MlflowClient()
          name = os.environ['MLFLOW_MODEL_NAME']
          stg = os.environ['FROM_STAGE']
          ver = client.get_latest_versions(name, [stg])[0].version
          client.transition_model_version_stage(name, ver, os.environ['TO_STAGE'], archive_existing_versions=True)
          print(f"Promoted {name} v{ver} -> {os.environ['TO_STAGE']}")
          PY

  rollback:
    runs-on: self-hosted
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.rollback_version != '' }}
    env:
      MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_URL }}
      MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_USER }}
      MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_PASS }}
    steps:
      - name: Show Python
        run: python3 -V
      - name: Set up venv
        run: |
          python3 -m venv .venv
          .venv/bin/python -m pip install -U pip
      - name: Rollback Production to specific version
        if: ${{ env.MLFLOW_TRACKING_URI != '' }}
        run: |
          .venv/bin/python -m pip install -q mlflow
          mlflow models transition-stage --model-name "${{ inputs.rollback_model }}" --version "${{ inputs.rollback_version }}" --stage Production
